---
title: "Human Activity Recognition Project"
author: "Tell Bennett"
output: html_document
---



### Synopsis

Using a random forest model and 10-fold cross-validation, we can predict the quality
of the dumbbell lift exercise with >99% accuracy. The estimated out-of-sample
accuracy is 100%, but that seems unlikely to hold in all cases.


```{r options, echo = FALSE}
options(dplyr.max_print = 10)
options(dplyr.min_print =  5)
set.seed(2342)
```

  

### Load R packages

```{r loadpackages, echo = TRUE, results="hide", message=FALSE, warning=FALSE}
library(MASS)
library(dplyr)
library(caret)
library(readr)
library(magrittr)
```

  
  
  

### Load the raw data

```{r dataload}
## Download the train data 
trainurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(trainurl, destfile = "data/pml-training.csv")

## Document the download time
traindownloadtime <- date()
traindownloadtime



## Download the train data 
testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(testurl, destfile = "data/pml-testing.csv")

## Document the download time
testdownloadtime <- date()
testdownloadtime

```

  
  
  

### Read the data into R

```{r dataread, message = FALSE, warning = FALSE}
training <- read_csv("data/pml-training.csv")
testing <- read_csv("data/pml-testing.csv")

### the below are for console use only
# training <- read_csv("PracticalMachineLearning/data/pml-training.csv")
# testing <- read_csv("PracticalMachineLearning/data/pml-testing.csv")
```

  
  
  

```{r explore1, echo = FALSE, results="hide", message=FALSE}
dim(training)
dim(testing)

glimpse(training)
names(training)
head(training)
tail(training)
summary(training)  ### a number of the variables have substantial missing data
```



### Remove columns with overwhelming amounts of missing data
```{r select1, results = "hide", message = FALSE}

### turning empty character strings into NA
training[training==""] <- NA 
testing[testing==""] <- NA 

### removing the columns with lots (more than 19000/19622 total rows) of missing data
training <- training[, sapply(training, function(x) sum(is.na(x))) < 19000]
testing <- testing[, sapply(testing, function(x) sum(is.na(x))) < 10]   ### no column has more than 1 NA now

### amount of missing data in remaining rows 
training <- training[complete.cases(training),]  ## - only 1 excluded
testing <- testing[complete.cases(testing),]  ## - none excluded

### amount of missing data in remaining columns - none
sapply(training, function(x) sum(is.na(x)))
sapply(testing, function(x) sum(is.na(x)))

### explore data again
glimpse(training)
glimpse(testing)
```

  
  
  

### Cleaning userid and outcome
```{r clean1}
training %<>%
     mutate(
        class = factor(classe),
        user = factor(user_name))

### Examining execution class by user
table(training$class,training$user)

```
Overall, the classes are reasonably balanced, so overall accuracy is likely an acceptable
performance measure for the prediction model.
  
  
  
  
### Exploring time variables
```{r time1}

training %>%
     dplyr::select(user, contains("cvtd")) %>%
     group_by(user) %>%
     table(.)
```
The training data were generated by observing 6 individuals lift the weight. Each person
was observed only once, and each person's observations were taken over 2-3 minutes on one day.
Therefore, I'll consider each person's data to have come from a single event, and will
not consider time variables as potential predictors. 

Because we do not have data about the individuals("users"), it will not be useful
to include "user" ID as a predictor, so I drop that variable.



### Removing unnecessary time variables and user ID
```{r select2}
training %<>%
     dplyr::select(-c(1:7),-classe,-user,-user_name)

testing %<>%
     dplyr::select(-c(1:7),-problem_id)
```



9) Pairs plots
```{r pairsplots, cache=TRUE}

### Belt variables
training %>%
     select(contains("belt"), class) %>%
     pairs(.)

### Arm variables - not shown, fewer apparent potential predictors
# training %>%
#      select(contains("_arm"), class) %>%
#      pairs(.)

### Dumbbell variables
training %>%
     select(contains("dumbbell"), class) %>%
     pairs(.)

### Forearm variables
training %>%
     select(contains("forearm"), class) %>%
     pairs(.)
```

On gross inspection, `roll_belt`, `yaw_belt`, and the `accel_belt` components appear to be potentially useful predictors.


### Looking for correlation among predictors and trimming predictors
```{r corr, cache = TRUE}

M <- training %>%
     dplyr::select(-class) %>%
     cor(.) %>%
     abs(.)

diag(M) <- 0

which(M > 0.9,arr.ind=T)

training %<>%
     dplyr::select(-accel_belt_y, -accel_belt_z, -gyros_arm_y, -gyros_forearm_z)

testing %<>%
     dplyr::select(-accel_belt_y, -accel_belt_z, -gyros_arm_y, -gyros_forearm_z)
```
Many predictors are highly correlated (r > 0.9). To simply the prediction process and improve interpretability, I excluded the following variables.
In particular, `total_accel_belt` is highly correlated with `accel_belt_y` and `accel_belt_z`, but not with `accel_belt_x`, so I excluded the `y` and `z` variables and kept `total` and `x`. `gyros_arm_x` and `gyros_arm_y` are highly correlated, so I kept `x`. `gyros_dumbbell_x` and `gyros_dumbbell_z` and `gyros_forearm_z` were all highly correlated, so I kept
only `gyros_forearm_z`.

  

### Fitting a LDA model
```{r lda1, cache=TRUE, results = "hide"}

ldafit <- train(class ~ ., data = training, preProcess = "scale", method="lda")
```


### LDA model accuracy
```{r lda2}

ldafit   ### 70% accuracy
```
The LDA model fits quickly, but unfortunately does not predict well in-sample.



### Testing a tree model
```{r tree1, cache = TRUE, results = "hide"}
treefit <- train(class ~ ., data = training, preProcess = "scale", method="rpart")
```


### Tree model accuracy
```{r tree2}

treefit   ### 51% accuracy
```
A basic classification tree is even worse.


### Pre-processing for RF model
```{r prerf, cache = TRUE, warning = FALSE, message = FALSE}

preProcValues <- preProcess(training, method = "scale")
trainproc <- predict(preProcValues, training)
testproc <- predict(preProcValues, testing)
```


### Testing a RF model
```{r rf1, cache = TRUE, warning = FALSE, message = FALSE}

control <- trainControl(method = "cv", number = 10)

noy <- trainproc[,-49]

outcome <- training$class

rffit <- train(x = noy, 
               y = outcome, 
               method="rf",
               trControl = control)
```
After pre-processing by scaling all of the continuous variables, I fit a random forest model with 10-fold cross-validation.



### RF model fit and variable importance
```{r rf2, cache = TRUE, message = FALSE}
rffit

rffit$finalModel

plot(varImp(rffit), top = 20)
```

This model has excellent accuracy, with an in-sample accuracy of 99.95% and an estimated out of sample error rate of 0%. That seems potentially overfit. `roll_belt` is the most important variable in the model, as the pairs plot suggested.



### Predictions
```{r predict1, cache = TRUE}
identical(names(noy),names(testproc))
```
I cannot figure out why these have different columns. Everything, including the below code chunk, will run in the console, and that work generated the 20 predicted classes for the other part of the project. For some reason, it won't run in `knitr`. 

```{r test}

# testresults <- predict(rffit, newdata = testproc)
# testresults
# 
# testchar <- as.character(testresults)
```

