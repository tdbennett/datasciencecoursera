---
output: html_document
---
## Notes from B Caffo week 4 lectures


### Power

* Power is the probability of rejecting the null hypothesis when it is false
* Ergo, power (as its name would suggest) is a good thing; you want more power
* A type II error (a bad thing, as its name would suggest) is failing to reject the null hypothesis when it's false; the probability of a type II error is usually called $\beta$
* Note Power $= 1 - \beta$

## Notes

* Consider our previous example involving RDI
* $H_0: \mu = 30$ versus $H_a: \mu > 30$
* Then power is $$P\left(\frac{\bar X - 30}{s /\sqrt{n}} > t_{1-\alpha,n-1} ~;~ \mu = \mu_a \right)$$
* Note that this is a function that depends on the specific value of $\mu_a$!
* Notice as $\mu_a$ approaches $30$ the power approaches $\alpha$

### Calculating power for Gaussian data

* We reject if $\frac{\bar X - 30}{\sigma /\sqrt{n}} > z_{1-\alpha}$
    - Equivalently if $\bar X > 30 + Z_{1-\alpha} \frac{\sigma}{\sqrt{n}}$
* Under $H_0 : \bar X \sim N(\mu_0, \sigma^2 / n)$
* Under $H_a : \bar X \sim N(\mu_a, \sigma^2 / n)$
* So we want...


### Example 

* $\mu_a = 32$, $\mu_0 = 30$, $n =16$, $\sigma = 4$

```{r ch2}
mu0 = 30; mua = 32; sigma = 4; n = 16
alpha = 0.05
z = qnorm(1 - alpha)
pnorm(mu0 + z * sigma / sqrt(n), mean = mu0, sd = sigma / sqrt(n), 
      lower.tail = FALSE)  ### should just return alpha
pnorm(mu0 + z * sigma / sqrt(n), mean = mua, sd = sigma / sqrt(n), 
      lower.tail = FALSE)  ### gives you power
```

### Plotting the power curve

```{r ch3}
library(ggplot2)
nseq = c(8, 16, 32, 64, 128)
mua = seq(30, 35, by = 0.1)
z = qnorm(.95)
power = sapply(nseq, function(n)
pnorm(mu0 + z * sigma / sqrt(n), mean = mua, sd = sigma / sqrt(n), 
          lower.tail = FALSE)
    )
colnames(power) <- paste("n", nseq, sep = "")
d <- data.frame(mua, power)
library(reshape2)
d2 <- melt(d, id.vars = "mua")
names(d2) <- c("mua", "n", "power")    
g <- ggplot(d2, 
            aes(x = mua, y = power, col = n)) + geom_line(size = 2)
g            
```

### Graphical Depiction of Power  - don't run using knitr, run separately
### could this be adapted to other designs?

```{r ch4, eval=FALSE}
library(manipulate)
mu0 = 30
myplot <- function(sigma, mua, n, alpha){
    g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
    g = g + stat_function(fun=dnorm, geom = "line", 
                          args = list(mean = mu0, sd = sigma / sqrt(n)), 
                          size = 2, col = "red")
    g = g + stat_function(fun=dnorm, geom = "line", 
                          args = list(mean = mua, sd = sigma / sqrt(n)), 
                          size = 2, col = "blue")
    xitc = mu0 + qnorm(1 - alpha) * sigma / sqrt(n)
    g = g + geom_vline(xintercept=xitc, size = 3)
    g
}
manipulate(
    myplot(sigma, mua, n, alpha),
    sigma = slider(1, 10, step = 1, initial = 4),
    mua = slider(30, 35, step = 1, initial = 32),
    n = slider(1, 50, step = 1, initial = 16),
    alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05)
    )
```

### Question

* When testing $H_a : \mu > \mu_0$, notice if power is $1 - \beta$, then $$1 - \beta = P\left(\bar X > \mu_0 + z_{1-\alpha} \frac{\sigma}{\sqrt{n}} ; \mu = \mu_a \right)$$
* where $\bar X \sim N(\mu_a, \sigma^2 / n)$
* Unknowns: $\mu_a$, $\sigma$, $n$, $\beta$
* Knowns: $\mu_0$, $\alpha$
* Specify any 3 of the unknowns and you can solve for the remainder

### Notes

* The calculation for $H_a:\mu < \mu_0$ is similar
* For $H_a: \mu \neq \mu_0$ calculate the one sided power using $\alpha / 2$ (this is only approximately right, it excludes the probability of getting a large TS in the opposite direction of the truth)
* Power goes up as $\alpha$ gets larger
* Power of a one sided test is greater than the power of the associated two sided test
* Power goes up as $\mu_1$ gets further away from $\mu_0$
* Power goes up as $n$ goes up
* Power doesn't need $\mu_a$, $\sigma$ and $n$, instead only $\frac{\sqrt{n}(\mu_a - \mu_0)}{\sigma}$
    - The quantity $\frac{\mu_a - \mu_0}{\sigma}$ is called the __effect size__, the difference in the means in standard deviation units.
    - Being unit free, it has some hope of interpretability across settings

### T-test power
-  Consider calculating power for a Gossett's $T$ test for our example
-  The power is
  $$
  P\left(\frac{\bar X - \mu_0}{S /\sqrt{n}} > t_{1-\alpha, n-1} ~;~ \mu = \mu_a \right)
  $$
- Calcuting this requires the non-central t distribution.
- `power.t.test` does this very well
  - Omit one of the arguments and it solves for it

### Example
```{r}
power.t.test(n = 16, delta = 2 / 4, sd=1, type = "one.sample",  alt = "one.sided")$power
power.t.test(n = 16, delta = 2, sd=4, type = "one.sample",  alt = "one.sided")$power
power.t.test(n = 16, delta = 100, sd=200, type = "one.sample", alt = "one.sided")$power
```

---
### Example
```{r}
power.t.test(power = .8, delta = 2 / 4, sd=1, type = "one.sample",  alt = "one.sided")$n
power.t.test(power = .8, delta = 2, sd=4, type = "one.sample",  alt = "one.sided")$n
power.t.test(power = .8, delta = 100, sd=200, type = "one.sample", alt = "one.sided")$n
```


### Types of errors

* Suppose you are testing a hypothesis that a parameter $\beta$ equals zero versus the alternative that it does not equal zero. These are the possible outcomes.

	$\beta=0$ 	$\beta\neq0$ 	Hypotheses
Claim $\beta=0$ 	$U$ 	$T$ 	$m-R$
Claim $\beta\neq 0$ 	$V$ 	$S$ 	$R$
Claims 	$m_0$ 	$m-m_0$ 	$m$

* Type I error or false positive ($V$) Say that the parameter does not equal zero when it does

* Type II error or false negative ($T$) Say that the parameter equals zero when it doesn't


### Error rates

* False positive rate - The rate at which false results ($\beta = 0$) are called significant: $E\left[\frac{V}{m_0}\right]$

* Family wise error rate (FWER) - The probability of at least one false positive ${\rm Pr}(V \geq 1)$

* False discovery rate (FDR) - The rate at which claims of significance are false $E\left[\frac{V}{R}\right]$

    - The false positive rate is closely related to the type I error rate http://en.wikipedia.org/wiki/False_positive_rate


### Controlling the false positive rate

* If P-values are correctly calculated calling all $P < \alpha$ significant will control the false positive rate at level $\alpha$ on average.

* Problem: Suppose that you perform 10,000 tests and $\beta = 0$ for all of them.

    - Suppose that you call all $P < 0.05$ significant.

    - The expected number of false positives is: $10,000 \times 0.05 = 500$ false positives.

    - How do we avoid so many false positives?


### Controlling family-wise error rate (FWER)

* The Bonferroni correction is the oldest multiple testing correction.

* Basic idea:

    Suppose you do $m$ tests
    You want to control FWER at level $\alpha$ so $Pr(V \geq 1) < \alpha$
    Calculate P-values normally
    Set $\alpha_{fwer} = \alpha/m$
    Call all $P$-values less than $\alpha_{fwer}$ significant

* Pros: Easy to calculate, conservative 
* Cons: May be very conservative

### Controlling false discovery rate (FDR)

* This is the most popular correction when performing lots of tests say in genomics, imaging, astronomy, or other signal-processing disciplines.

* Basic idea:

    Suppose you do $m$ tests
    You want to control FDR at level $\alpha$ so $E\left[\frac{V}{R}\right]$
    Calculate P-values normally
    Order the P-values from smallest to largest $P_{(1)},...,P_{(m)}$
    Call any $P_{(i)} \leq \alpha \times \frac{i}{m}$ significant

* Pros: Still pretty easy to calculate, less conservative (maybe much less)

* Cons: Allows for more false positives, may behave strangely under dependence

### Adjusted P-values

* One approach is to adjust the threshold $\alpha$
* A different approach is to calculate "adjusted p-values"
* They _are not p-values_ anymore
* But they can be used directly without adjusting $\alpha$

__Example__: 
* Suppose P-values are $P_1,\ldots,P_m$
* You could adjust them by taking $P_i^{fwer} = \max{m \times P_i,1}$ for each P-value.
* Then if you call all $P_i^{fwer} < \alpha$ significant you will control the FWER. 

---

### Case study I: no true positives

```{r createPvals,cache=TRUE}
set.seed(1010093)
pValues <- rep(NA,1000)
for(i in 1:1000){
  y <- rnorm(20)
  x <- rnorm(20)
  pValues[i] <- summary(lm(y ~ x))$coeff[2,4]  ### nice trick for getting p-value out of model
}

# Controls false positive rate
sum(pValues < 0.05)
```

---

### Case study I: no true positives

```{r, dependson="createPvals"}
# Controls FWER 
sum(p.adjust(pValues,method="bonferroni") < 0.05)
# Controls FDR 
sum(p.adjust(pValues,method="BH") < 0.05)
```


---

### Case study II: 50% true positives

```{r createPvals2,cache=TRUE}
set.seed(1010093)
pValues <- rep(NA,1000)
for(i in 1:1000){
  x <- rnorm(20)
  # First 500 beta=0, last 500 beta=2
  if(i <= 500){y <- rnorm(20)}else{ y <- rnorm(20,mean=2*x)}
  pValues[i] <- summary(lm(y ~ x))$coeff[2,4]
}
trueStatus <- rep(c("zero","not zero"),each=500)
table(pValues < 0.05, trueStatus)
```

```{r, dependson="createPvals2"}
# Controls FWER 
sum(p.adjust(pValues,method="bonferroni") < 0.05)
# Controls FDR 
sum(p.adjust(pValues,method="BH") < 0.05)
```

## Case study II: 50% true positives

__P-values versus adjusted P-values__
```{r, dependson="createPvals2",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
plot(pValues,p.adjust(pValues,method="bonferroni"),pch=19)
plot(pValues,p.adjust(pValues,method="BH"),pch=19)
```


---


## Notes and resources

__Notes__:
* Multiple testing is an entire subfield
* A basic Bonferroni/BH correction is usually enough
* If there is strong dependence between tests there may be problems
  * Consider method="BY"

__Further resources__:
* [Multiple testing procedures with applications to genomics](http://www.amazon.com/Multiple-Procedures-Applications-Genomics-Statistics/dp/0387493166/ref=sr_1_2/102-3292576-129059?ie=UTF8&s=books&qid=1187394873&sr=1-2)
* [Statistical significance for genome-wide studies](http://www.pnas.org/content/100/16/9440.full)
* [Introduction to multiple testing](http://ies.ed.gov/ncee/pubs/20084018/app_b.asp)


