---
output: html_document
---
## Notes from B Caffo week 1 lectures

### Probability Intro

prob'y refers to population, i.e. fundamental property of a die being rolled

3 rules are Kolmogorov's - see LittleInferenceBook, page 7

1) Pr gets a number between 0 and 1

2) Pr if something occurs is 1, if nothing, 0

3) If two events are mutually exclusive, the Pr is the sum of the prob's


Consequences:

* For any 2 events, the Pr that at least one occurs is the sum of the 2 Pr's minus the intersection



*Random variable* = numerical outcome of an experiment (TB: anything, BMI, a factor, count, whatever)



### Probability mass functions - for discrete

A PMF evaluated at a value corresponds to the Pr that a random variable takes *that value* 

A function p that is a PMF must always be >= 0, and the sum of all possible values must = 1

1) $p(x) \geq 0$ for all $x$
2) $\sum_{x} p(x) = 1$

    
* Most famous PMF: Bernoulli, results of coin flip

$$ p(x) = (1/2)^{x} (1/2)^{1-x} \mbox{ for }x = 0,1 $$ for x = 0,1 of fair coin
    
$$ p(x) = \theta^{x} (1 - \theta)^{1-x} \mbox{ for }x = 0,1 $$ for x = 0,1 of unfair coin
    
    we don't know theta, so we use our data to estimate it



### Probability density functions - for continuous
        
* Same rule: A function p that is a PDF must always be >= 0, and the total area under the PDF must = 1
* PDF applies to population, not the data (will use the data to estimate the population PDF)


Proportion of help calls that get addressed in a random day by a help line is 

$$ f(x) = \left{\begin{array}{ll} 2 x & \mbox{ for } 1 > x > 0 \ 0 & \mbox{ otherwise} \end{array} \right. $$

```{r pdf1}
x <- c(-0.5, 0, 1, 1, 1.5); y <- c( 0, 0, 2, 0, 0)
plot(x, y, lwd = 3, frame = FALSE, type = "l")

pbeta(0.75,2,1) ### beta distribution, pbeta gives Pr of being <0.75 from a beta distribution
```


### Cumulative Distribution Function (CDF) of a random variable X returns the Pr that the random variable is <= to the value x (discrete or continuous)

$$ F(x) = P(X \leq x) $$

p[density name] in R returns the CDF (e.g. pbeta)


* Survival function is 1 - CDF

$$ S(x) = P(X > x) $$
    



### Quantiles (still in PDF lecture)

$\alpha^{th}$ quantile of a distribution with distribution function $F$ is the point $x_\alpha$ so that 

$$ F(x_\alpha) = \alpha $$

qbeta(0.5, 2, 1) ### q[function name] gives population quantiles


### Linking Sample to a Population

estimand = target of estimation = population quantity (e.g. population median)
estimator = quantity in the sample (e.g. sample median)




### Conditional Prob Lecture


* Let $B$ be an event so that $P(B) > 0$
* Then the conditional probability of an event $A$ given that $B$ has occurred is $$ P(A ~|~ B) = \frac{P(A \cap B)}{P(B)} $$
* Notice that if $A$ and $B$ are independent (defined later in the lecture), then $$ P(A ~|~ B) = \frac{P(A) P(B)}{P(B)} = P(A) $$


### Mutually exclusive and not mutually exclusive

$$ \begin{eqnarray} A_1 & = & {\mbox{Person has sleep apnea}} \ A_2 & = & {\mbox{Person has RLS}} \end{eqnarray} $$

Then

$$ \begin{eqnarray} P(A_1 \cup A_2 ) & = & P(A_1) + P(A_2) - P(A_1 \cap A_2) \ & = & 0.13 - \mbox{Probability of having both} \end{eqnarray} $$ Likely, some fraction of the population has both.



### Bayes' Rule

Thomas Bayes, Presbyterian minister

Baye's rule allows us to reverse the conditioning set provided that we know some marginal probabilities $$ P(B ~|~ A) = \frac{P(A ~|~ B) P(B)}{P(A ~|~ B) P(B) + P(A ~|~ B^c)P(B^c)}. $$


* Let $+$ and $-$ be the events that the result of a diagnostic test is positive or negative respectively
* Let $D$ and $D^c$ be the event that the subject of the test has or does not have the disease respectively
* The sensitivity is the probability that the test is positive given that the subject actually has the disease, $P(+ ~|~ D)$
* The specificity is the probability that the test is negative given that the subject does not have the disease, $P(- ~|~ D^c)$


* The positive predictive value is the probability that the subject has the disease given that the test is positive, $P(D ~|~ +)$
* The negative predictive value is the probability that the subject does not have the disease given that the test is negative, $P(D^c ~|~ -)$
* The prevalence of the disease is the marginal probability of disease, $P(D)$



$$ \begin{eqnarray} P(D ~|~ +) & = &\frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}\ \ & = & \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + {1-P(-~|~D^c)}{1 - P(D)}} \ \ & = & \frac{.997\times .001}{.997 \times .001 + .015 \times .999}\ \ & = & .062 \end{eqnarray} $$



* The diagnostic likelihood ratio of a positive test, labeled $DLR_+$, is $P(+ ~|~ D) / P(+ ~|~ D^c)$, which is the $$sensitivity / (1 - specificity)$$
* The diagnostic likelihood ratio of a negative test, labeled $DLR_-$, is $P(- ~|~ D) / P(- ~|~ D^c)$, which is the $$(1 - sensitivity) / specificity$$


#### TB: like MIM paper, prior odds x LR = posterior odds

Using Bayes rule, we have $$ P(D ~|~ +) = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)} $$ and $$ P(D^c ~|~ +) = \frac{P(+~|~D^c)P(D^c)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}. $$


* Therefore $$ \frac{P(D ~|~ +)}{P(D^c ~|~ +)} = \frac{P(+~|~D)}{P(+~|~D^c)}\times \frac{P(D)}{P(D^c)} $$ ie $$ \mbox{post-test odds of }D = DLR_+\times\mbox{pre-test odds of }D $$
* Similarly, $DLR_-$ relates the decrease in the odds of the disease after a negative test result to the odds of disease prior to the test.



### Independence

* Two events $A$ and $B$ are independent if $$P(A \cap B) = P(A)P(B)$$
* Equivalently if $P(A ~|~ B) = P(A)$

* TB: Key issue is you can multiply prob'ies of independent events _only_, not correlated ones
    
* Two random variables, $X$ and $Y$ are independent if for any two sets $A$ and $B$ $$P([X \in A] \cap [Y \in B]) = P(X\in A)P(Y\in B)$$

If $A$ is independent of $B$ then
    * $A^c$ is independent of $B$
    * $A$ is independent of $B^c$
    * $A^c$ is independent of $B^c$
    
IID random variables = Independent, Identically Distributed
    * Ind: statistically unrelated
    * ID: drawn from the same population distribution


### Expected Values

* Expected values are properties of distributions
* The average of random var's is a random var and its associated distr'n has an expected value
        * the center of mass of this distr'n is the same as that of the original distr'n
        
Therefore, the center of the population distr'n of the sample mean is that same as the population mean
    * this is the definition of an _unbiased_ estimator
    
Example: 1000 standard normals, take the mean _vs_ 10 standard normals, take the mean, repeat that many times
    * the above two things will tend to have the same mean, but different variance
        * bigger groups (e.g. 30 standard normals at a time) > tighter variance around the mean
    * TB: is this the standard error of the mean vs SD issue?

We use mass and density functions (or their estimates) to talk about samples from populations
    * The mean is the center of mass - he uses the picture example for both discrete and continuous
    * variance, how spread out the mass is

sample mean is est. of population mean

* The sample mean estimates this population mean - TB look at the equation below
* The center of mass of the data is the empirical mean $$ \bar X = \sum_{i=1}^n x_i p(x_i) $$ where $p(x_i) = 1/n$


sample var is est. of population var
sample SD is est. of population SD

```{r centermass, message=FALSE}
library(UsingR); data(galton); library(ggplot2)
library(reshape2)
longGalton <- melt(galton, measure.vars = c("child", "parent"))
g <- ggplot(longGalton, aes(x = value)) + 
    geom_histogram(aes(y = ..density..,  fill = variable), binwidth=1, colour = "black") + 
    geom_density(size = 2)
g <- g + facet_grid(. ~ variable)
g
```

```{r centermass manipulate, eval=FALSE}
library(manipulate)
myHist <- function(mu){
    g <- ggplot(galton, aes(x = child))
    g <- g + geom_histogram(fill = "salmon", 
      binwidth=1, aes(y = ..density..), colour = "black")
    g <- g + geom_density(size = 2)
    g <- g + geom_vline(xintercept = mu, size = 2)
    mse <- round(mean((galton$child - mu)^2), 3)  
    g <- g + labs(title = paste('mu = ', mu, ' MSE = ', mse))
    g
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
```



