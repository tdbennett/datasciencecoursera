---
output: html_document
---
## Notes from B Caffo week 1 lectures

### Probability Intro

prob'y refers to population, i.e. fundamental property of a die being rolled

3 rules are Kolmogorov's - see LittleInferenceBook, page 7

1) Pr gets a number between 0 and 1

2) Pr if something occurs is 1, if nothing, 0

3) If two events are mutually exclusive, the Pr is the sum of the prob's


Consequences:

* For any 2 events, the Pr that at least one occurs is the sum of the 2 Pr's minus the intersection



*Random variable* = numerical outcome of an experiment (TB: anything, BMI, a factor, count, whatever)



### Probability mass functions - for discrete

A PMF evaluated at a value corresponds to the Pr that a random variable takes *that value* 

A function p that is a PMF must always be >= 0, and the sum of all possible values must = 1

1) $p(x) \geq 0$ for all $x$
2) $\sum_{x} p(x) = 1$

    
* Most famous PMF: Bernoulli, results of coin flip

$$ p(x) = (1/2)^{x} (1/2)^{1-x} \mbox{ for }x = 0,1 $$ for x = 0,1 of fair coin
    
$$ p(x) = \theta^{x} (1 - \theta)^{1-x} \mbox{ for }x = 0,1 $$ for x = 0,1 of unfair coin
    
    we don't know theta, so we use our data to estimate it



### Probability density functions - for continuous
        
* Same rule: A function p that is a PDF must always be >= 0, and the total area under the PDF must = 1
* PDF applies to population, not the data (will use the data to estimate the population PDF)


Proportion of help calls that get addressed in a random day by a help line is 

$$ f(x) = \left{\begin{array}{ll} 2 x & \mbox{ for } 1 > x > 0 \ 0 & \mbox{ otherwise} \end{array} \right. $$

```{r pdf1}
x <- c(-0.5, 0, 1, 1, 1.5); y <- c( 0, 0, 2, 0, 0)
plot(x, y, lwd = 3, frame = FALSE, type = "l")

pbeta(0.75,2,1) ### beta distribution, pbeta gives Pr of being <0.75 from a beta distribution
```


* Cumulative Distribution Function (CDF) of a random variable X returns the Pr that the random variable is <= to the value x (discrete or continuous)

$$ F(x) = P(X \leq x) $$

p[density name] in R returns the CDF (e.g. pbeta)


* Survival function is 1 - CDF

$$ S(x) = P(X > x) $$
    



### Quantiles (still in PDF lecture)

$\alpha^{th}$ quantile of a distribution with distribution function $F$ is the point $x_\alpha$ so that 

$$ F(x_\alpha) = \alpha $$

qbeta(0.5, 2, 1) ### q[function name] gives population quantiles


### Linking Sample to a Population

estimand = target of estimation = population quantity (e.g. population median)
estimator = quantity in the sample (e.g. sample median)




### Conditional Prob Lecture


* Let $B$ be an event so that $P(B) > 0$
* Then the conditional probability of an event $A$ given that $B$ has occurred is $$ P(A ~|~ B) = \frac{P(A \cap B)}{P(B)} $$
* Notice that if $A$ and $B$ are independent (defined later in the lecture), then $$ P(A ~|~ B) = \frac{P(A) P(B)}{P(B)} = P(A) $$



### Bayes' Rule

Thomas Bayes, Presbyterian minister

Baye's rule allows us to reverse the conditioning set provided that we know some marginal probabilities $$ P(B ~|~ A) = \frac{P(A ~|~ B) P(B)}{P(A ~|~ B) P(B) + P(A ~|~ B^c)P(B^c)}. $$


* Let $+$ and $-$ be the events that the result of a diagnostic test is positive or negative respectively
* Let $D$ and $D^c$ be the event that the subject of the test has or does not have the disease respectively
* The sensitivity is the probability that the test is positive given that the subject actually has the disease, $P(+ ~|~ D)$
* The specificity is the probability that the test is negative given that the subject does not have the disease, $P(- ~|~ D^c)$


* The positive predictive value is the probability that the subject has the disease given that the test is positive, $P(D ~|~ +)$
* The negative predictive value is the probability that the subject does not have the disease given that the test is negative, $P(D^c ~|~ -)$
* The prevalence of the disease is the marginal probability of disease, $P(D)$



$$ \begin{eqnarray} P(D ~|~ +) & = &\frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}\ \ & = & \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + {1-P(-~|~D^c)}{1 - P(D)}} \ \ & = & \frac{.997\times .001}{.997 \times .001 + .015 \times .999}\ \ & = & .062 \end{eqnarray} $$



* The diagnostic likelihood ratio of a positive test, labeled $DLR_+$, is $P(+ ~|~ D) / P(+ ~|~ D^c)$, which is the $$sensitivity / (1 - specificity)$$
* The diagnostic likelihood ratio of a negative test, labeled $DLR_-$, is $P(- ~|~ D) / P(- ~|~ D^c)$, which is the $$(1 - sensitivity) / specificity$$


#### TB: like MIM paper, prior odds x LR = posterior odds

Using Bayes rule, we have $$ P(D ~|~ +) = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)} $$ and $$ P(D^c ~|~ +) = \frac{P(+~|~D^c)P(D^c)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}. $$


* Therefore $$ \frac{P(D ~|~ +)}{P(D^c ~|~ +)} = \frac{P(+~|~D)}{P(+~|~D^c)}\times \frac{P(D)}{P(D^c)} $$ ie $$ \mbox{post-test odds of }D = DLR_+\times\mbox{pre-test odds of }D $$
* Similarly, $DLR_-$ relates the decrease in the odds of the disease after a negative test result to the odds of disease prior to the test.
