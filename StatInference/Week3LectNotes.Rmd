---
output: html_document
---
## Notes from B Caffo week 3 lectures


### T Confidence intervals

* In the previous, we discussed creating a confidence interval using the CLT
    - They took the form $Est \pm ZQ \times SE_{Est}$
*In this lecture, we discuss some methods for small samples, notably Gosset's $t$ distribution and $t$ confidence intervals
    - They are of the form $Est \pm TQ \times SE_{Est}$
* These are some of the handiest of intervals
* If you want a rule between whether to use a $t$ interval or normal interval, just always use the $t$ interval
* We'll cover the one and two group versions

### Gosset's $t$ distribution

* Invented by William Gosset (under the pseudonym "Student") in 1908
* Has thicker tails than the normal
* Is indexed by a degrees of freedom; gets more like a standard normal as df gets larger 
* It assumes that the underlying data are iid Gaussian with the result that $$ \frac{\bar X - \mu}{S/\sqrt{n}} $$ follows Gosset's $t$ distribution with $n-1$ degrees of freedom
    
* (If we replaced $s$ by $\sigma$ the statistic would be exactly standard normal)
* Interval is $\bar X \pm t_{n-1} S/\sqrt{n}$ where $t_{n-1}$ is the relevant quantile

### Code for manipulate - not run

```{r tplot, eval=FALSE}
library(ggplot2)
library(manipulate)
k <- 1000
xvals <- seq(-5, 5, length = k)
myplot <- function(df){
  d <- data.frame(y = c(dnorm(xvals), dt(xvals, df)),
                  x = xvals,
                  dist = factor(rep(c("Normal", "T"), c(k,k))))
  g <- ggplot(d, aes(x = x, y = y)) 
  g <- g + geom_line(size = 2, aes(colour = dist))
  g
}
manipulate(myplot(mu), mu = slider(1, 20, step = 1))  
```


### Easier to see - not run

```{r quantilesofthet, eval=FALSE}
pvals <- seq(.5, .99, by = .01)
myplot2 <- function(df){
  d <- data.frame(n= qnorm(pvals),t=qt(pvals, df),
                  p = pvals)
  g <- ggplot(d, aes(x= n, y = t))
  g <- g + geom_abline(size = 2, col = "lightblue")
  g <- g + geom_line(size = 2, col = "black")
  g <- g + geom_vline(xintercept = qnorm(0.975))
  g <- g + geom_hline(yintercept = qt(0.975, df))
  g
}
manipulate(myplot2(df), df = slider(1, 20, step = 1))
```

### Notes about the $t$ interval

* The $t$ interval technically assumes that the data are iid normal, though it is robust to this assumption
* It works well whenever the distribution of the data is roughly symmetric and mound shaped
* Paired observations are often analyzed using the $t$ interval by taking differences
* For large degrees of freedom, $t$ quantiles become the same as standard normal quantiles; therefore this interval converges to the same interval as the CLT yielded
    
* For skewed distributions, the spirit of the $t$ interval assumptions are violated
    - Also, for skewed distributions, it doesn't make a lot of sense to center the interval at the mean
    - In this case, consider taking logs or using a different summary like the median
* For highly discrete data, like binary, other intervals are available

### Sleep data

In R typing data(sleep) brings up the sleep data originally analyzed in Gosset's Biometrika paper, which shows the increase in hours for 10 patients on two soporific drugs. R treats the data as two groups rather than paired.

The data

```{r sleep1}
data(sleep)
head(sleep)
```

### Plotting the data

```{r sleepplot}
library(ggplot2)
g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))
g <- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = "salmon", alpha = .5)
g
```

### Results - manual calculation of t confidence interval

```{r sleepresults}
g1 <- sleep$extra[1 : 10]; g2 <- sleep$extra[11 : 20]
difference <- g2 - g1
mn <- mean(difference); s <- sd(difference); n <- 10

mn + c(-1, 1) * qt(.975, n-1) * s / sqrt(n)
t.test(difference)
t.test(g2, g1, paired = TRUE)
t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)
```


### The results

(After a little formatting)

```{r moresleep}
rbind(
mn + c(-1, 1) * qt(.975, n-1) * s / sqrt(n),
as.vector(t.test(difference)$conf.int),
as.vector(t.test(g2, g1, paired = TRUE)$conf.int),
as.vector(t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)$conf.int)
)
```

### Independent group $t$ confidence intervals

* Suppose that we want to compare the mean blood pressure between two groups in a randomized trial; those who received the treatment to those who received a placebo

* We cannot use the paired t test because the groups are independent and may have different sample sizes
* We now present methods for comparing independent groups

### Confidence interval

* Therefore a $(1 - \alpha)\times 100\%$ confidence interval for $\mu_y - \mu_x$ is $$ \bar Y - \bar X \pm t_{n_x + n_y - 2, 1 - \alpha/2}S_p\left(\frac{1}{n_x} + \frac{1}{n_y}\right)^{1/2} $$

* The pooled variance estimator is $$S_p^2 = {(n_x - 1) S_x^2 + (n_y - 1) S_y^2}/(n_x + n_y - 2)$$
* Remember this interval is assuming a constant variance across the two groups
* If there is some doubt, assume a different variance per group, which we will discuss later

### Example
Based on Rosner, Fundamentals of Biostatistics

(Really a very good reference book)

* Comparing SBP for 8 oral contraceptive users versus 21 controls
* $\bar X_{OC} = 132.86$ mmHg with $s_{OC} = 15.34$ mmHg
* $\bar X_{C} = 127.44$ mmHg with $s_{C} = 18.23$ mmHg
* Pooled variance estimate

```{r xyz}
sp <- sqrt((7 * 15.34^2 + 20 * 18.23^2) / (8 + 21 - 2))
132.86 - 127.44 + c(-1, 1) * qt(.975, 27) * sp * (1 / 8 + 1 / 21)^.5
```

### Mistakenly treating the sleep data as grouped

```{r more4}
n1 <- length(g1)
n2 <- length(g2)
sp <- sqrt( ((n1 - 1) * sd(g1)^2 + (n2-1) * sd(g2)^2) / (n1 + n2-2))
md <- mean(g2) - mean(g1)
semd <- sp * sqrt(1 / n1 + 1/n2)

rbind(
md + c(-1, 1) * qt(.975, n1 + n2 - 2) * semd,  
t.test(g2, g1, paired = FALSE, var.equal = TRUE)$conf,
t.test(g2, g1, paired = TRUE)$conf
)
```

### Grouped versus independent

```{r grpind}
library(ggplot2)
g <- ggplot(sleep, aes(x = group, y = extra, group = factor(ID)))
g <- g + geom_line(size = 1, aes(colour = ID)) + geom_point(size =10, pch = 21, fill = "salmon", alpha = .5)
g
```

### ChickWeight data in R

```{r chickwt} 
library(datasets) 
data(ChickWeight) 
library(reshape2)
##define weight gain or loss
wideCW <- dcast(ChickWeight, Diet + Chick ~ Time, value.var = "weight")
names(wideCW)[-(1 : 2)] <- paste("time", names(wideCW)[-(1 : 2)], sep = "")
library(dplyr)
wideCW <- mutate(wideCW,
  gain = time21 - time0
)

### Plotting the raw data - spaghetti plot

g <- ggplot(ChickWeight, aes(x = Time, y = weight, 
                             colour = Diet, group = Chick))
g <- g + geom_line()
g <- g + stat_summary(aes(group = 1), geom = "line", fun.y = mean, size = 1, col = "black")
g <- g + facet_grid(. ~ Diet)
g
```

### Weight gain by diet

```{r bydiet}
g <- ggplot(wideCW, aes(x = factor(Diet), y = gain, fill = factor(Diet)))
g <- g + geom_violin(col = "black", size = 2)
g
```

### Let's do a t interval

```{r chickt}
wideCW14 <- subset(wideCW, Diet %in% c(1, 4))
rbind(
t.test(gain ~ Diet, paired = FALSE, var.equal = TRUE, data = wideCW14)$conf,
t.test(gain ~ Diet, paired = FALSE, var.equal = FALSE, data = wideCW14)$conf
)
```

### Unequal variances

* Under unequal variances $$ \bar Y - \bar X \pm t_{df} \times \left(\frac{s_x^2}{n_x} + \frac{s_y^2}{n_y}\right)^{1/2} $$ where $t_{df}$ is calculated with degrees of freedom $$ df= \frac{\left(S_x^2 / n_x + S_y^2/n_y\right)^2} {\left(\frac{S_x^2}{n_x}\right)^2 / (n_x - 1) + \left(\frac{S_y^2}{n_y}\right)^2 / (n_y - 1)} $$ will be approximately a 95% interval

* This works really well
    - So when in doubt, just assume unequal variances

### Example

* Comparing SBP for 8 oral contraceptive users versus 21 controls
    $\bar X_{OC} = 132.86$ mmHg with $s_{OC} = 15.34$ mmHg
    $\bar X_{C} = 127.44$ mmHg with $s_{C} = 18.23$ mmHg
    $df=15.04$, $t_{15.04, .975} = 2.13$
    Interval $$ 132.86 - 127.44 \pm 2.13 \left(\frac{15.34^2}{8} + \frac{18.23^2}{21} \right)^{1/2} = [-8.91, 19.75] $$
    In R, t.test(..., var.equal = FALSE)

### Comparing other kinds of data

* For binomial data, there's lots of ways to compare two groups
    - Relative risk, risk difference, odds ratio.
    - Chi-squared tests, normal approximations, exact tests.
    - For count data, there's also Chi-squared tests and exact tests.
* We'll leave the discussions for comparing groups of data for binary and count data until covering glms in the regression class.
* In addition, Mathematical Biostatistics Boot Camp 2 covers many special cases relevant to biostatistics.

### Hypothesis testing

* Hypothesis testing is concerned with making decisions using data
* A null hypothesis is specified that represents the status quo, usually labeled $H_0$
* The null hypothesis is assumed true and statistical evidence is required to reject it in favor of a research or alternative hypothesis

### Example

* A respiratory disturbance index of more than $30$ events / hour, say, is considered evidence of severe sleep disordered breathing (SDB).
* Suppose that in a sample of $100$ overweight subjects with other risk factors for sleep disordered breathing at a sleep clinic, the mean RDI was $32$ events / hour with a standard deviation of $10$ events / hour.
* We might want to test the hypothesis that
        $H_0 : \mu = 30$
        $H_a : \mu > 30$
        where $\mu$ is the population mean RDI.

### Hypothesis testing

* The alternative hypotheses are typically of the form $<$, $>$ or $\neq$
* Note that there are four possible outcomes of our statistical decision process

Truth     Decide 	Result
$H_0$ 	$H_0$ 	Correctly accept null
$H_0$ 	$H_a$ 	Type I error
$H_a$ 	$H_a$ 	Correctly reject null
$H_a$ 	$H_0$ 	Type II error

### Discussion

* Consider a court of law; the null hypothesis is that the defendant is innocent
* We require a standard on the available evidence to reject the null hypothesis (convict)
* If we set a low standard, then we would increase the percentage of innocent people convicted (type I errors); however we would also increase the percentage of guilty people convicted (correctly rejecting the null)
* If we set a high standard, then we increase the the percentage of innocent people let free (correctly accepting the null) while we would also increase the percentage of guilty people let free (type II errors)

### Example

* Consider our sleep example again
* A reasonable strategy would reject the null hypothesis if $\bar X$ was larger than some constant, say $C$
* Typically, $C$ is chosen so that the probability of a Type I error, $\alpha$, is $.05$ (or some other relevant constant)
* $\alpha$ = Type I error rate = Probability of rejecting the null hypothesis when, in fact, the null hypothesis is correct

### Example continued

* Standard error of the mean $10 / \sqrt{100} = 1$
* Under $H_0$ $\bar X \sim N(30, 1)$
* We want to chose $C$ so that the $P(\bar X > C; H_0)$ is 5%
* The 95th percentile of a normal distribution is 1.645 standard deviations from the mean
* If $C = 30 + 1 \times 1.645 = 31.645$
    - Then the probability that a $N(30, 1)$ is larger than it is 5%
    - So the rule "Reject $H_0$ when $\bar X \geq 31.645$" has the property that the probability of rejection is 5% when $H_0$ is true (for the $\mu_0$, $\sigma$ and $n$ given)

### Discussion

* In general we don't convert $C$ back to the original scale
* We would just reject because the Z-score; which is how many standard errors the sample mean is above the hypothesized mean $$ \frac{32 - 30}{10 / \sqrt{100}} = 2 $$ is greater than $1.645$
* Or, whenever $\sqrt{n} (\bar X - \mu_0) / s > Z_{1-\alpha}$

