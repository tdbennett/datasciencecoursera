---
output: html_document
---
## Notes from B Caffo week 2 lectures

### Intro to Variability

### Population variance

If $X$ is a random variable with mean $\mu$, the variance of $X$ is defined as

$$ Var(X) = E[(X - \mu)^2] = E[X^2] - E[X]^2 $$ 

The square root of the variance is called the standard deviation
The standard deviation has the same units as $X$


Ex. 1.  What's the variance from the result of a toss of a die?

$E[X] = 3.5$
    
$E[X^2] = 1 ^ 2 \times \frac{1}{6} + 2 ^ 2 \times \frac{1}{6} + 3 ^ 2 \times \frac{1}{6} + 4 ^ 2 \times \frac{1}{6} + 5 ^ 2 \times \frac{1}{6} + 6 ^ 2 \times \frac{1}{6} = 15.17$

$$Var(X) = E[X^2] - E[X]^2 \approx 2.92$$


Ex. 2.  What's the variance from the result of a biased coin flip - probability of heads (1) of $p$?

$E[X] = 0 \times (1 - p) + 1 \times p = p$

$E[X^2] = E[X] = p$

$$Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1 - p)$$  ### binomial distribution


```{r varfigure, cache=TRUE}
library(ggplot2)
xvals <- seq(-10, 10, by = .01)
dat <- data.frame(
    y = c(
        dnorm(xvals, mean = 0, sd = 1),
        dnorm(xvals, mean = 0, sd = 2),
        dnorm(xvals, mean = 0, sd = 3),
        dnorm(xvals, mean = 0, sd = 4)
    ),
    x = rep(xvals, 4),
    factor = factor(rep(1 : 4, rep(length(xvals), 4)))
)
ggplot(dat, aes(x = x, y = y, color = factor)) + geom_line(size = 2) 
```


### Sample Variance

The sample variance is $$ S^2 = \frac{\sum_{i=1} (X_i - \bar X)^2}{n-1} $$ (almost, but not quite, the average squared deviation from the sample mean)

* It is also a random variable
* It has an associate population distribution
* Its expected value is the population variance
* Its distribution gets more concentrated around the population variance with more data
* Its square root is the sample standard deviation

#### Simulating from a population with variance 1

```{r sim, cache=TRUE}
nosim <- 10000; 
dat <- data.frame(
    x = c(apply(matrix(rnorm(nosim * 10), nosim), 1, var),
          apply(matrix(rnorm(nosim * 20), nosim), 1, var),
          apply(matrix(rnorm(nosim * 30), nosim), 1, var)),
    n = factor(rep(c("10", "20", "30"), c(nosim, nosim, nosim))) 
    )
ggplot(dat, aes(x = x, fill = n)) + geom_density(size = 2, alpha = .2) + geom_vline(xintercept = 1, size = 2) 
```

#### Variances of x die rolls

```{r dierolls, cache=TRUE}
dat <- data.frame(
  x = c(apply(matrix(sample(1 : 6, nosim * 10, replace = TRUE), 
                     nosim), 1, var),
        apply(matrix(sample(1 : 6, nosim * 20, replace = TRUE), 
                     nosim), 1, var),
        apply(matrix(sample(1 : 6, nosim * 30, replace = TRUE), 
                     nosim), 1, var)
        ),
  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))
g <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.3, colour = "black") 
g <- g + geom_vline(xintercept = 2.92, size = 2)
g + facet_grid(. ~ size)
```

### Recall the mean

* Recall that the average of random sample from a population is itself a random variable
* We know that this distribution is centered around the population mean, $E[\bar X] = \mu$
* We also know what its variance is $Var(\bar X) = \sigma^2 / n$
* This is very useful, since we don't have repeat sample means to get its variance; now we know how it relates to the population variance
* We call the standard deviation of a statistic a standard error

### To summarize

* The sample variance, $S^2$, estimates the population variance, $\sigma^2$
* The distribution of the sample variance is centered around $\sigma^2$
* The variance of sample mean is $\sigma^2 / n$
    - Its logical estimate is $s^2 / n$
    - The logical estimate of the standard error is $s / \sqrt{n}$
* $s$, the standard deviation, talks about how variable the population is
* $s/\sqrt{n}$, the standard error, talks about how variable averages of random samples of size $n$ from the population are

### Simulation example - standard normal

* Standard normals have variance 1; means of $n$ standard normals have standard deviation $1/\sqrt{n}$

```{r simstnorm, cache=TRUE}
nosim <- 1000
n <- 10
sd(apply(matrix(rnorm(nosim * n), nosim), 1, mean)) ### SD
1 / sqrt(n) ### SEM - very close
```

### Simulation example - standard uniform

* Standard uniforms have variance $1/12$; means of random samples of $n$ uniforms have sd $1/\sqrt{12 \times n}$

```{r simstunif, cache=TRUE} 
nosim <- 1000
n <- 10
sd(apply(matrix(runif(nosim * n), nosim), 1, mean)) 
1 / sqrt(12 * n)
```

### Simulation example - Poisson

* Poisson(4) have variance $4$; means of random samples of $n$ Poisson(4) have sd $2/\sqrt{n}$

```{r simpoisson, cache=TRUE}
nosim <- 1000
n <- 10
sd(apply(matrix(rpois(nosim * n, 4), nosim), 1, mean))
2 / sqrt(n)
```

### Simulation example - coin flips

* Fair coin flips have variance $0.25$; means of random samples of $n$ coin flips have sd $1 / (2 \sqrt{n})$

```{r simcoinflip, cache=TRUE}
nosim <- 1000
n <- 10
sd(apply(matrix(sample(0 : 1, nosim * n, replace = TRUE),
                nosim), 1, mean))
1 / (2 * sqrt(n))
```


## Variance Data Example lecture

### Data example

```{r height1}
library(UsingR); data(father.son); 
x <- father.son$sheight
n<-length(x)
```

### Plot of the son's heights

```{r height2}
g <- ggplot(data = father.son, aes(x = sheight)) 
g <- g + geom_histogram(aes(y = ..density..), fill = "lightblue", binwidth=1, colour = "black")
g <- g + geom_density(size = 2, colour = "black")
g
```

### Let's interpret these numbers

```{r height3}
round(c(var(x), var(x) / n, sd(x), sd(x) / sqrt(n)),2)
## [1] 7.92 0.01 2.81 0.09
g
```

### Summarizing what we know about variances

* The sample variance estimates the population variance
* The distribution of the sample variance is centered at what its estimating
* It gets more concentrated around the population variance with larger sample sizes
* The variance of the sample mean is the population variance divided by $n$
    - The square root is the standard error
* It turns out that we can say a lot about the distribution of averages from random samples, even though we only get one look in a given data set



## Common Distibutions

### The Bernoulli distribution

* The Bernoulli distribution arises as the result of a binary outcome
* Bernoulli random variables take (only) the values 1 and 0 with probabilities of (say) $p$ and $1-p$ respectively
* The PMF for a Bernoulli random variable $X$ is $$P(X = x) = p^x (1 - p)^{1 - x}$$
* The mean of a Bernoulli random variable is $p$ and the variance is $p(1 - p)$
* If we let $X$ be a Bernoulli random variable, it is typical to call $X=1$ as a "success" and $X=0$ as a "failure"

### Binomial trials

* The binomial random variables are obtained as the sum of iid Bernoulli trials
* In specific, let $X_1,\ldots,X_n$ be iid Bernoulli$(p)$; then $X = \sum_{i=1}^n X_i$ is a binomial random variable
* The binomial mass function is $$ P(X = x) = \left( \begin{array}{c} n \ x \end{array} \right) p^x(1 - p)^{n-x} $$ for $x=0,\ldots,n$

### Choose

* Recall that the notation $$\left( \begin{array}{c} n \ x \end{array} \right) = \frac{n!}{x!(n-x)!} $$ (read "$n$ choose $x$") counts the number of ways of selecting $x$ items out of $n$ without replacement disregarding the order of the items

$$\left( \begin{array}{c} n \ 0 \end{array} \right) = \left( \begin{array}{c} n \ n \end{array} \right) = 1 $$

### Example

* Suppose a friend has $8$ children (oh my!), $7$ of which are girls and none are twins
* If each gender has an independent $50$% probability for each birth, what's the probability of getting $7$ or more girls out of $8$ births? $$\left( \begin{array}{c} 8 \ 7 \end{array} \right) .5^{7}(1-.5)^{1} + \left( \begin{array}{c} 8 \ 8 \end{array} \right) .5^{8}(1-.5)^{0} \approx 0.04 $$

```{r choose}
choose(8, 7) * .5 ^ 8 + choose(8, 8) * .5 ^ 8 
pbinom(6, size = 8, prob = .5, lower.tail = FALSE)
```


### The normal distribution

* A random variable is said to follow a normal or Gaussian distribution with mean $\mu$ and variance $\sigma^2$ if the associated density is $$ (2\pi \sigma^2)^{-1/2}e^{-(x - \mu)^2/2\sigma^2} $$ 
* If $X$ a RV with this density then $E[X] = \mu$ and $Var(X) = \sigma^2$
    - We write $X\sim \mbox{N}(\mu, \sigma^2)$
    - When $\mu = 0$ and $\sigma = 1$ the resulting distribution is called the standard normal distribution
    - Standard normal RVs are often labeled $Z$

### The standard normal distribution with reference lines

```{r normal}
x <- seq(-3, 3, length = 1000)
library(ggplot2)
g <- ggplot(data.frame(x = x, y = dnorm(x)), 
            aes(x = x, y = y)) + geom_line(size = 2)
g <- g + geom_vline(xintercept = -3 : 3, size = 2)
g
```

### Facts about the normal density

* If $X \sim \mbox{N}(\mu,\sigma^2)$ then $$Z = \frac{X -\mu}{\sigma} \sim N(0, 1)$$
    - from non-standard normal to Z scores (standard normal)

* If $Z$ is standard normal $$X = \mu + \sigma Z \sim \mbox{N}(\mu, \sigma^2)$$
    - back from Z score to non-standard normal

### More facts about the normal density

* Approximately $68\%$, $95\%$ and $99\%$ of the normal density lies within $1$, $2$ and $3$ standard deviations from the mean, respectively
* $-1.28$, $-1.645$, $-1.96$ and $-2.33$ are the $10^{th}$, $5^{th}$, $2.5^{th}$ and $1^{st}$ percentiles of the standard normal distribution respectively
* By symmetry, $1.28$, $1.645$, $1.96$ and $2.33$ are the $90^{th}$, $95^{th}$, $97.5^{th}$ and $99^{th}$ percentiles of the standard normal distribution respectively

### Question

* What is the $95^{th}$ percentile of a $N(\mu, \sigma^2)$ distribution?
    - Quick answer in R qnorm(.95, mean = mu, sd = sd)
* Or, because you have the standard normal quantiles memorized and you know that 1.645 is the 95th percentile you know that the answer has to be $$\mu + \sigma 1.645$$
    - (In general $\mu + \sigma z_0$ where $z_0$ is the appropriate standard normal quantile)

### Question

* What is the probability that a $\mbox{N}(\mu,\sigma^2)$ RV is larger than $x$?

```{r pnorm}
## pnorm(x, mean = mu, sd = sigma, lower.tail=FALSE) 
#OR
```
$x - \mu / \sigma$ is how many SD's from the mean x is

### Example

Assume that the number of daily ad clicks for a company is (approximately) normally distributed with a mean of 1020 and a standard deviation of 50. What's the probability of getting more than 1,160 clicks in a day?

It's not very likely, 1,160 is r (1160 - 1020) / 50 standard deviations from the mean

```{r ex12}
pnorm(1160, mean = 1020, sd = 50, lower.tail = FALSE)
pnorm(2.8, lower.tail = FALSE)
```

### Example

Assume that the number of daily ad clicks for a company is (approximately) normally distributed with a mean of 1020 and a standard deviation of 50. What number of daily ad clicks would represent the one where 75% of days have fewer clicks (assuming days are independent and identically distributed)?

TB: one SD above and below includes 68%, therefore 16% in each tail, therefore 75% below overall would be between the mean
and one SD above

```{r qnorm}
qnorm(0.75, mean = 1020, sd = 50) ### gives 1054, within one SD above
```

### The Poisson distribution

* Used to model counts
* The Poisson mass function is $$ P(X = x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!} $$ for $x=0,1,\ldots$
* The mean of this distribution is $\lambda$
* The variance of this distribution is $\lambda$
* Notice that $x$ ranges from $0$ to $\infty$

### Some uses for the Poisson distribution

* Modeling count data
* Modeling event-time or survival data
* Modeling contingency tables
* Approximating binomials when $n$ is large and $p$ is small 
    - common in epi

### Rates and Poisson random variables

* Poisson random variables are used to model rates
* $X \sim Poisson(\lambda t)$ where
    - $\lambda = E[X / t]$ is the expected count per unit of time
    - $t$ is the total monitoring time

### Example

The number of people that show up at a bus stop is Poisson with a mean of $2.5$ per hour.

If watching the bus stop for 4 hours, what is the probability that $3$ or fewer people show up for the whole time?

```{r pois}
ppois(3, lambda = 2.5 * 4)
```

### Poisson approximation to the binomial

* When $n$ is large and $p$ is small the Poisson distribution is an accurate approximation to the binomial distribution
* Notation
    - $X \sim \mbox{Binomial}(n, p)$
    - $\lambda = n p$
    - $n$ gets large
    - $p$ gets small

### Example, Poisson approximation to the binomial

We flip a coin with success probablity $0.01$ five hundred times.

What's the probability of 2 or fewer successes?

```{r pois2}
pbinom(2, size = 500, prob = .01)
ppois(2, lambda=500 * .01)
```